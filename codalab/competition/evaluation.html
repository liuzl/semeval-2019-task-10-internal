<h2>Task 10: Math Question Answering</h2>
<h3>Evaluation</h3>
<p><span>Evaluation will be based solely on a system's ability to answer questions correctly.</span></p>
<p><span>For each subtask, the main evaluation metric will simply be question <strong>accuracy</strong>, i.e. the number of correctly answered questions. The evaluation script takes as input a list of JSON datum </span><span>{ </span><span>id: </span><span>&lt;</span><span>id</span><span>&gt;</span><span>, answer: "</span><span>&lt;</span><span>response</span><span>&gt;"</span><span>}</span><span>, where </span><span>&lt;</span><span>id</span><span>&gt; </span><span>is the integer index of a question and </span><span>&lt;</span><span>response</span><span>&gt; </span><span>is the guessed response (either a choice key or a numeric string). It will output the system&rsquo;s score as the number of correct responses divided by the total number of questions in the subtask. </span></p>
<p><span>While the main evaluation metric includes no penalties for guessing, we will also compute a secondary metric called <strong>penalized accuracy</strong> that implements the actual evaluation metric used to score these SATs. This metric is the number of correct questions, minus 1/4 point for each incorrect guess. We include this metric to challenge participants to investigate high-precision QA systems. </span></p>
